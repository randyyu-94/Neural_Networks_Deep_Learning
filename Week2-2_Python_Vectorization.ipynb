{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section II Python and Vectorization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Vectorization\n",
    "\n",
    "##### (1) Example of calculating *z* in logistic regression\n",
    "$z = w^Tx + b, \\mathrm{where:} w = \\left[\\begin{array}{c}\n",
    "w_1\\\\\n",
    "w_2\\\\\n",
    "\\vdots\\\\\n",
    "w_{n_x}\n",
    "\\end{array}\\right], x = \\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_{n_x}\n",
    "\\end{array}\\right]; w \\in \\textbf{R}^{n_{x}}, x \\in \\textbf{R}^{n_{x}}$\n",
    "\n",
    "- **For non-vectorized (i.e. *for* loop) case**:\n",
    "    - 1. $z = 0$\n",
    "    - 2. for *i* in range($n_x$):\n",
    "        - 3-1. $z\\ +=\\ w^{(i)}x^{(i)}$\n",
    "    - 4. $z\\ +=\\ b$\n",
    "- **for vectorized case**:\n",
    "    - 1. $z = \\mathrm{np.dot}(w,x)+b$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Vectorization saving time (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 249869.8724567009, Vectorized version:0.9975433349609375ms\n"
     ]
    }
   ],
   "source": [
    "# Get the computation time with vectorized method np.dot\n",
    "tic = time.time()\n",
    "c = np.dot(a,b)\n",
    "toc = time.time()\n",
    "\n",
    "print('c = ' + str(c) + ', Vectorized version:' +str(1000*(toc-tic)) + 'ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 249869.87245669932, Non-vectorized version:328.11975479125977ms\n"
     ]
    }
   ],
   "source": [
    "# Get the computation time with non-vectorized method for loop \n",
    "tic = time.time()\n",
    "c = 0\n",
    "for ai, bi in zip(a,b):\n",
    "    c += ai*bi\n",
    "toc = time.time()\n",
    "print('c = ' + str(c) + ', Non-vectorized version:' + str(1000*(toc-tic))+'ms')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparing the two codes: vectorization speeds up more than **300** times!\n",
    "- Using the internal functions (like *np.dot*) can enable python numpy to use parallelism methods to compute with GPU and CPU much faster.\n",
    "- **Rule of thumb**: whenever possible, avoid explicit *for* loops."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Other vectorization examples\n",
    "$\\boxed{\\textbf{u}_{m} = \\textbf{A}_{mn}\\textbf{v}_{n}}$\n",
    "\n",
    "- **For non-vectorized (i.e. *for* loop) case**:\n",
    "    - 1. $\\textbf{u} = \\mathrm{np.zeros}((n,1))$\n",
    "    - 2. *for* $i\\ \\mathrm{in}\\ \\mathrm{range}(m)$:\n",
    "        - 3. *for* $j\\ \\mathrm{in}\\ \\mathrm{range}(n)$:\n",
    "            - 4. $\\textbf{u}_i\\ += \\textbf{A}_{ij}*\\textbf{v}_j$\n",
    "- **for vectorized case**:\n",
    "    - 1. $\\textbf{u} = \\mathrm{np.dot}(\\textbf{A},\\textbf{v})$  \n",
    "$\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad$  \n",
    "\n",
    "$\\boxed{\\textbf{v} =  \\left[\\begin{array}{c}\n",
    "v_1\\\\\n",
    "v_2\\\\\n",
    "\\vdots\\\\\n",
    "v_n\n",
    "\\end{array}\\right] \\Longrightarrow \\textbf{u} = \\left[\\begin{array}{c}\n",
    "e^{v_1}\\\\\n",
    "e^{v_2}\\\\\n",
    "\\vdots\\\\\n",
    "e^{v_n}\n",
    "\\end{array}\\right]}$\n",
    "- **For non-vectorized (i.e. *for* loop) case**:\n",
    "    - 1. $\\textbf{u} = \\mathrm{np.zeros}((n,1))$\n",
    "    - 2. *for* $v_i\\ \\mathrm{in}\\ \\textbf{v}$:\n",
    "        - 3. $u_i = e^{v_i}$\n",
    "- **for vectorized case**:\n",
    "    - 1. $\\textbf{u} = \\mathrm{np.exp}(\\textbf{v})$\n",
    "\n",
    "$\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad$\n",
    "- **Other vectorization codes**:\n",
    "    - np.log(v) - computes natural logarithm of every elements in *v*\n",
    "    - np.abs(v) - computes the absolute value of every elements in *v*\n",
    "    - np.maximum(u,v) - compares every elements in *u* and *v* and provides the maximum of every pair\n",
    "    - v**2 - computes the square of every elements in *v*\n",
    "    - 1/v - computes the inverse of every elements in *v*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Vectorizing Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **Original code** (from ***Week2-1**: 7. Logistic Regression Gradient Descent*)\n",
    ">    - 1. Set: $J = 0, \\mathrm{d}w_1 = 0, \\mathrm{d}w_2 = 0, \\mathrm{d}b = 0$\n",
    ">    - 2. *for* $i = 1\\ to\\ m$:\n",
    ">        - 3-1. $z^{(i)} = w^Tx^{(i)}+b$\n",
    ">        - 3-2. $a^{(i)} = \\sigma (z^{(i)})$\n",
    ">        - 3-3. $J\\ +=\\ y^{(i)}\\log a^{(i)} + (1-y^{(i)})\\log(1-a^{(i)}) $\n",
    ">        - 3-4. $\\mathrm{d}z^{(i)} = a^{(i)} - y^{(i)}$\n",
    ">        - 3-5. $\\mathrm{d}w_1\\ +=\\ x_1^{(i)}\\mathrm{d}z^{(i)}$\n",
    ">        - 3-6. $\\mathrm{d}w_2\\ +=\\ x_2^{(i)}\\mathrm{d}z^{(i)}$ - If more than 2 features: another *for* loop\n",
    ">        - 3-7. $\\mathrm{d}b\\ +=\\ \\mathrm{d}z^{(i)}$\n",
    ">    - 4-1. $J := J/m$ - Key step: get *J* value to determine the performance\n",
    ">    - 4-2. $\\mathrm{d}w_1 := \\mathrm{d}w_1/m$\n",
    ">    - 4-3. $\\mathrm{d}w_2 := \\mathrm{d}w_2/m$\n",
    ">    - 4-4. $\\mathrm{d}b := \\mathrm{d}b/m$\n",
    ">    - 5-1. $w_1 := w_1 - \\alpha\\mathrm{d}w_1$\n",
    ">    - 5-2. $w_2 := w_2 - \\alpha\\mathrm{d}w_2$\n",
    ">    - 5-3. $b := b - \\alpha\\mathrm{d}b$\n",
    "\n",
    "- **Vectorizing *for* loop for multiple features**: \n",
    "    - Combine $w_1, w_2, ..., w_n$ into $\\textbf{w}$ and $x_1, x_2, ..., x_n$ into $\\textbf{x}$, and compute $\\mathrm{d}\\textbf{w}\\ += \\textbf{x}^{(i)}\\mathrm{d}z^{(i)}$  \n",
    "    \n",
    "$\\boxed{z^{i}\\ =\\ w^Tx^{(i)}+b, a^{(i)}\\ =\\ \\sigma(z^{(i)})}$, *for* $i\\ \\mathrm{in\\ range}(m)$\n",
    "- **Vectorizing logistic regression prediction *a***\n",
    "    - $\\textbf{z} = [z^{(1)}\\ z^{(2)}\\ ...\\ z^{(m)}],\\textbf{X} = [x^{(1)}\\ x^{(2)}\\ ...\\ x^{(m)}]\\Rightarrow \\textbf{z} = w^T\\textbf{X}+\\textbf{b},\\ where:\\ \\textbf{b}=[b\\ b\\ ...\\ b],1\\times m$\n",
    "    - $[z^{(1)}\\ z^{(2)}\\ ...\\ z^{(m)}] = [w_1\\ w_2\\ ...\\ w_n]\\left[\\begin{array}{c}\n",
    "x_1^{(1)} & x_1^{(2)} & ... & x_1^{(m)}\\\\\n",
    "x_2^{(1)} & x_2^{(2)} & ... & x_2^{(m)}\\\\\n",
    "\\vdots & \\vdots & ... & \\vdots\\\\\n",
    "x_n^{(1)} & x_n^{(2)} & ... & x_n^{(m)}\n",
    "\\end{array}\\right]\\ +[b\\ b\\ ...\\ b] = [\\textbf{w}^T\\textbf{x}^{(1)}+b\\ \\ \\textbf{w}^T\\textbf{x}^{(2)}+b\\ \\ ...\\ \\ \\textbf{w}^T\\textbf{x}^{(m)}+b]$\n",
    "        - **Python code**: Z = np.dot(w.T, X) + b (python expands *b* to 1*m vector and add it to the vector; **broadcasting**)\n",
    "    - $\\textbf{a} = \\sigma(\\textbf{z})$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Broadcasting in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
