{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module 3: Shallow Neural Network**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Neural network representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) One hidden layer neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Single Hidden Layer](resource%20database%20for%20MD%20notes/Week3/SingleHiddenLayer.jpg)\n",
    "- **Definition**: A neural network structure that contains a hidden layer prior to its output layer (also called **2-layer network**)\n",
    "- **Terms**:\n",
    "    - Input layer (*layer 0*): $x_1, x_2, x_3 \\Rightarrow X = a^{[0]}$\n",
    "    - Hidden layer (*layer 1*): $a^{[1]}_1,a^{[1]}_2,a^{[1]}_3,a^{[1]}_4 \\Rightarrow a^{[1]}$\n",
    "        - *Hidden* indicates that the true values of these nodes are **not observed**.\n",
    "    - Output layer (*layer 2*): $\\hat{y} = a^{[2]}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Computing a neural network's output on a *single* sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each node of *layer 1* $a_i^{[1]}$:\n",
    "$$\\boxed{z_i^{[1]} = \\textbf{w}_i^{[1]T}\\textbf{x} + b_i^{[1]}, a_i^{[1]} = \\sigma{z_i^{[1]}}}$$\n",
    "where: *superscipt* $^{[1]}$ indicates the No. of *layer*, *subscript* $_i$ indicates the No. of the *node* in this *layer*\n",
    "- For the full *layer 1*:\n",
    "\n",
    "$$\\boxed{\\left\\{\\begin{array}{c}\n",
    "z_1^{[1]} = \\textbf{w}_1^{[1]T}\\textbf{x} + b_1^{[1]}\\\\\n",
    "z_2^{[1]} = \\textbf{w}_2^{[1]T}\\textbf{x} + b_2^{[1]}\\\\\n",
    "z_3^{[1]} = \\textbf{w}_3^{[1]T}\\textbf{x} + b_3^{[1]}\\\\\n",
    "z_4^{[1]} = \\textbf{w}_4^{[1]T}\\textbf{x} + b_4^{[1]}\n",
    "\\end{array}\\right\\}\\Rightarrow \\left\\{\\begin{array}{c}\n",
    "a_1^{[1]} = \\sigma{z_1^{[1]}}\\\\\n",
    "a_2^{[1]} = \\sigma{z_2^{[1]}}\\\\\n",
    "a_3^{[1]} = \\sigma{z_3^{[1]}}\\\\\n",
    "a_4^{[1]} = \\sigma{z_4^{[1]}}\n",
    "\\end{array}\\right\\}}$$\n",
    "- **Vectorization of the calculation**:\n",
    "$$\\boxed{\\left[\\begin{array}{c}\n",
    "z_1^{[1]}\\\\\n",
    "z_2^{[1]}\\\\\n",
    "z_3^{[1]}\\\\\n",
    "z_4^{[1]}\n",
    "\\end{array}\\right] =  \n",
    "\\left[\\begin{array}{c}\n",
    "(w_1^{[1]})_{x_1} & (w_1^{[1]})_{x_2} & (w_1^{[1]})_{x_3}\\\\\n",
    "(w_2^{[1]})_{x_1} & (w_2^{[1]})_{x_2} & (w_2^{[1]})_{x_3}\\\\\n",
    "(w_3^{[1]})_{x_1} & (w_3^{[1]})_{x_2} & (w_3^{[1]})_{x_3}\\\\\n",
    "(w_4^{[1]})_{x_1} & (w_4^{[1]})_{x_2} & (w_4^{[1]})_{x_3}\\\\\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_3\n",
    "\\end{array}\\right] + \\left[\\begin{array}{c}\n",
    "b_1^{[1]}\\\\\n",
    "b_2^{[1]}\\\\\n",
    "b_3^{[1]}\\\\\n",
    "b_4^{[1]}\n",
    "\\end{array}\\right]}$$\n",
    "$$\\boxed{\\textbf{z}^{[1]} = \\textbf{w}^{[1]T}\\textbf{x} + \\textbf{b}^{[1]}}$$\n",
    "where: $\\textbf{z}\\mathrm{.shape} = (4,1), \\textbf{w}^{[1]T}\\mathrm{.shape} = (4,3), \\textbf{x}\\mathrm{.shape} = (3,1), \\textbf{b}\\mathrm{.shape} = (4,1)$  \n",
    "$$\\boxed{\\left[\\begin{array}{c}\n",
    "a_1^{[1]}\\\\\n",
    "a_2^{[1]}\\\\\n",
    "a_3^{[1]}\\\\\n",
    "a_4^{[1]}\n",
    "\\end{array}\\right] = \\sigma\\left[\\begin{array}{c}\n",
    "z_1^{[1]}\\\\\n",
    "z_2^{[1]}\\\\\n",
    "z_3^{[1]}\\\\\n",
    "z_4^{[1]}\n",
    "\\end{array}\\right]}$$\n",
    "$$\\boxed{\\textbf{a}^{[1]} = \\sigma(\\textbf{z}^{[1]})}$$\n",
    "where: $\\textbf{a}\\mathrm{.shape} = \\textbf{z}\\mathrm{.shape} = (4,1)$\n",
    "- For the full *layer 2*:\n",
    "$$\\boxed{z^{[2]} = \\textbf{w}^{[2]T}\\textbf{a}^{[1]} + b^{[2]}\\Rightarrow a^{[2]} = \\sigma(z^{[2]})}$$\n",
    "where: $\\textbf{w}^{[2]T}\\mathrm{.shape} = (1,4), \\textbf{a}^{[1]}\\mathrm{.shape} = (4,1)$\n",
    "- **Vectorization of the calculation**:\n",
    "$$\\boxed{z^{[2]} = \\left[\\begin{array}{c}\n",
    "w_1^{[2]} & w_2^{[2]} & w_3^{[2]} & w_4^{[2]}\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "a_1^{[1]}\\\\\n",
    "a_2^{[1]}\\\\\n",
    "a_3^{[1]}\\\\\n",
    "a_4^{[1]}\n",
    "\\end{array}\\right]+b^{[2]}}$$\n",
    "- For the sample:\n",
    "$$\\textbf{z}^{[1]} = \\textbf{w}^{[1]T}\\textbf{x} + \\textbf{b}^{[1]}$$\n",
    "$$\\textbf{a}^{[1]} = \\sigma(\\textbf{z}^{[1]})$$\n",
    "$$z^{[2]} = \\textbf{w}^{[2]T}\\textbf{a}^{[1]} + b^{[2]}$$\n",
    "$$a^{[2]} = \\sigma(z^{[2]})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Vectorizing across *multiple* examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For *m* samples:\n",
    "    - Denotation of a *node*: $a_l^{[i](j)}$  \n",
    "where: *i* denotes the No. of the *layer*, *j* denotes the No. of the *sample*, and *l* denotes the No. of the *node* in this layer\n",
    "    - *For* loop for all samples:\n",
    "        - *for i = 1 to m*:\n",
    "            - $z^{[1](i)} = w^{[1]T}x^{(i)}+b^{[1]}$\n",
    "            - $a^{[1](i)} = \\sigma(z^{[1](i)})$\n",
    "            - $z^{[2](i)} = w^{[2]T}a^{[1](i)}+b^{[2]}$\n",
    "            - $a^{[2](i)} = \\sigma(z^{[2](i)})$\n",
    "- **Vectorization of the calculation**:\n",
    "$$\\boxed{\\textbf{X} = \\left[\\begin{array}{c}\n",
    "x_1^{(1)} & x_1^{(2)} & ... & x_1^{(m)}\\\\\n",
    "x_2^{(1)} & x_2^{(2)} & ... & x_2^{(m)}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "x_{n_x}^{(1)} & x_{n_x}^{(2)} & ... & x_{n_x}^{(m)}\n",
    "\\end{array}\\right], \\textbf{X}.\\mathrm{shape} = (n_x,m)}$$\n",
    "$$\\boxed{\\textbf{w} = \\left[\\begin{array}{c}\n",
    "w_1^{(1)} & w_1^{(2)} & ... & w_1^{(n_1)}\\\\\n",
    "w_2^{(1)} & w_2^{(2)} & ... & w_2^{(n_1)}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "w_{n_x}^{(1)} & w_{n_x}^{(2)} & ... & w_{n_x}^{(n_1)}\n",
    "\\end{array}\\right], \\textbf{w}.\\mathrm{shape} = (n_x,n_1)}$$\n",
    "By converting $x^{(1)}$ - $x^{(m)}$ to $\\textbf{X}$, we can recompute the equations for all *m* samples:\n",
    "$$\\textbf{Z}^{[1]} = \\textbf{w}^{[1]T}\\textbf{X} + \\textbf{b}^{[1]}$$\n",
    "$$\\textbf{A}^{[1]} = \\sigma(\\textbf{Z}^{[1]})$$\n",
    "where: $\\textbf{A}^{[1]}.\\mathrm{shape} = \\textbf{Z}^{[1]}.\\mathrm{shape} = (n_1,m), \\textbf{w}^{[1]T}.\\mathrm{shape} = (n_1,n_x),\\textbf{X}.\\mathrm{shape} = (n_x,m),\\textbf{b}^{[1]}.\\mathrm{shape} = (n_1,m)( \\mathrm{broadcasted}\\ \\mathrm{from}\\ (n_1,1))$, $n_1$ is the total number of *nodes* in *layer* 1 (i.e., 4 in this example).\n",
    "\n",
    "$$\\textbf{Z}^{[2]} = \\textbf{w}^{[2]T}\\textbf{A}^{[1]} + b^{[2]}$$\n",
    "$$\\textbf{A}^{[2]} = \\sigma(\\textbf{Z}^{[2]})$$\n",
    "where: $\\textbf{A}^{[2]}.\\mathrm{shape} = \\textbf{Z}^{[2]}.\\mathrm{shape} = (1,m), \\textbf{w}^{[2]T}.\\mathrm{shape} = (1,n_1),\\textbf{X}.\\mathrm{shape} = (n_1,m),\\textbf{b}^{[2]}.\\mathrm{shape} = (1,m)( \\mathrm{broadcasted}\\ \\mathrm{from}\\ (1,1))$, assuming *layer* 2 is the *output layer* where $\\textbf{A}^{[2]} = \\hat{\\textbf{Y}}$\n",
    "- **Further explanation for vectorized implementation**:\n",
    "$$\\textbf{w}^{[1]T}\\textbf{X} = \\left[\\begin{array}{c}\n",
    "w_1^{[1](1)} & w_2^{[1](1)} & ... & w_{n_x}^{[1](1)}\\\\\n",
    "w_1^{[1](2)} & w_2^{[1](2)} & ... & w_{n_x}^{[1](2)}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "w_1^{[1](k_1)} & w_2^{[1](k_1)} & ... & w_{n_x}^{[1](k_1)}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "x_1^{(1)} & x_1^{(2)} & ... & x_1^{(m)}\\\\\n",
    "x_2^{(1)} & x_2^{(2)} & ... & x_2^{(m)}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "x_{n_x}^{(1)} & x_{n_x}^{(2)} & ... & x_{n_x}^{(m)}\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "\\sum\\limits_{i=1}^{n_x}w_i^{[1](1)}x_i^{(1)} & \\sum\\limits_{i=1}^{n_x}w_i^{[1](1)}x_i^{(2)} & ... & \\sum\\limits_{i=1}^{n_x}w_i^{[1](1)}x_i^{(m)}\\\\\n",
    "\\sum\\limits_{i=1}^{n_x}w_i^{[1](2)}x_i^{(1)} & \\sum\\limits_{i=1}^{n_x}w_i^{[1](2)}x_i^{(2)} & ... & \\sum\\limits_{i=1}^{n_x}w_i^{[1](2)}x_i^{(m)}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "\\sum\\limits_{i=1}^{n_x}w_i^{[1](k_1)}x_i^{(1)} & \\sum\\limits_{i=1}^{n_x}w_i^{[1](k_1)}x_i^{(2)} & ... & \\sum\\limits_{i=1}^{n_x}w_i^{[1](k_1)}x_i^{(m)}\\\\\n",
    "\\end{array}\\right]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Activation Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) General activation functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Definition**: an *activation function* of a *node* (*g*) calculates the <u>output</u> of the *node* based on a set of given <u>input</u>\n",
    "    $$\\boxed{a^{[i]} = g^{[i]}(z^{[i]}), z^{[i]} = W^{[i]}a^{[i-1]} + b^{[i]}}$$\n",
    "    - Can be different for different layers\n",
    "\n",
    "- **General activation functions**\n",
    "    - *sigmoid* function\n",
    "        - Worst performance among all *activation functions*\n",
    "        - Generally only used for the *output layer* of binary classification models (at the $\\hat{y}$ output)\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"resource%20database%20for%20MD%20notes/Week2/1280px-Logistic-curve.svg.png\" alt=\"sigmoid\">\n",
    "        </div>\n",
    "\n",
    "        $$\n",
    "        \\boxed{a = \\sigma(z) = \\frac{1}{1+e^{-z}}}\n",
    "        $$\n",
    "\n",
    "    - *hyperbolic tangent (tanh)* function\n",
    "        - <u>Mathematically shifted</u> *sigmoid* function\n",
    "        - works better than *sigmoid*, especially for 0-centered datasets - the *mean* is closer to 0\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"resource%20database%20for%20MD%20notes/Week3/TanhReal.gif\" alt=\"hyperbolic tangent\">\n",
    "        </div>\n",
    "        \n",
    "        $$\n",
    "        \\boxed{a = \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}}\n",
    "        $$\n",
    "        - Drawback of *sigmoid* and *tanh* function\n",
    "            - When *z* is very **large** or very **small**, the slope (gradient of the derivative) of the function is very **small**, thus affecting the performance of *gradient descent*\n",
    "\n",
    "\n",
    "    - *rectified linear unit (ReLU)* function\n",
    "        - the derivative is:\n",
    "            - 1 for $z > 0$ and 0 for $z < 0$\n",
    "            - 0.000... (simulation: 0 or 1) when $z = 0$\n",
    "        - increasingly the <u>default</u> choice of *activation function* for middle layers\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"resource%20database%20for%20MD%20notes/Week1/ezgif-3-e2770704ae.jpg\" alt=\"ReLU\">\n",
    "        </div>\n",
    "\n",
    "        $$\n",
    "        \\boxed{a = \\max(0,z)}\n",
    "        $$\n",
    "        - Drawback of *ReLU* function\n",
    "            - the slope of the function in the negative zone is 0, which results in loss of training results when $x < 0$\n",
    "    - *Leaky ReLU* function\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"resource%20database%20for%20MD%20notes/Week3/leakyrelu.png\" alt=\"leaky relu\">\n",
    "        </div>\n",
    "\n",
    "        $$\n",
    "        \\boxed{a = \\max(0.01z,z)}\n",
    "        $$\n",
    "        - Advantage of *ReLU* and *Leaky ReLU* function\n",
    "            - the derivative is much higher than that for *sigmoid* and *tanh* functions when *z* is very **large** or very **small**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Rationale of using non-linear activation functions for neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Why a neural network needs a non-linear activation function?**\n",
    "- **Identity activation function**:\n",
    "    $$\n",
    "    \\boxed{a^{[i]} = z^{[i]}, z^{[i]} = W^{[i]}a^{[i-1]} + b^{[i]}}\n",
    "    $$\n",
    "    - Finally...\n",
    "    $$\n",
    "    \\boxed{\\hat{y} = W^{[l]}(W^{[l-1]}(...(W^{[1]}x + b^{[1]}) + ...) + + b^{[l-1]}) + b^{[l]} = \\prod_{i=1}^{l} W^{[i]}x + \\sum\\limits_{i=1}^{l}(\\prod_{j=i+1}^{l} W^{[j]} b^{[i]}) = Wx + b}\n",
    "    $$\n",
    "    - Result: the predicted output $\\hat{y}$ is always a linear function of input features no matter how many *layers* we use\n",
    "    - If *identity activation function* is applied in all *hidden layers* while the output layer uses another *activation function*:\n",
    "        - The model is equivalent to a model using the last *activation function* without any *hidden layers*\n",
    "    - *identity activation function* Can only be used at the *output layer* for real number computation (e.g., housing price vs. features)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Gradient Descent for Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Derivatives of activation functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For *sigmoid* function:\n",
    "    $$\n",
    "    \\boxed{\n",
    "        \\!\\begin{aligned}\n",
    "        &\\qquad\\qquad\\qquad g(z) = \\frac{1}{1+e^{-z}}\\\\\n",
    "        &\\frac{\\mathrm{d}g(z)}{dz} = \\frac{1}{1+e^{-z}}(1-\\frac{1}{1+e^{-z}}) = g(z) (1-g(z))\n",
    "        \\end{aligned}\n",
    "        }\n",
    "    $$\n",
    "    - When $z$ is very **large** (say $z = 10$): $g(z)\\rightarrow 1 \\Rightarrow \\frac{\\mathrm{d}g(z)}{dz} \\rightarrow 0 $ \n",
    "    - When $z$ is very **small** (say $z = -10$): $g(z)\\rightarrow 0 \\Rightarrow \\frac{\\mathrm{d}g(z)}{dz} \\rightarrow 0 $ \n",
    "    - When $z$ is **close to 0** (say $z = 0$): $g(z) = \\frac{1}{2} \\Rightarrow \\frac{\\mathrm{d}g(z)}{dz} = \\frac{1}{4} $ \n",
    "\n",
    "- For *tanh* function:\n",
    "    $$\n",
    "    \\boxed{\n",
    "        \\!\\begin{aligned}\n",
    "        &\\qquad g(z) = \\tanh (z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}\\\\\n",
    "        &\\frac{\\mathrm{d}g(z)}{dz} = 1 - (\\frac{e^z-e^{-z}}{e^z+e^{-z}})^2 = 1 - {g(z)}^2\n",
    "        \\end{aligned}\n",
    "    }\n",
    "    $$\n",
    "    \n",
    "    - When $z$ is very **large** (say $z = 10$): $g(z)\\rightarrow 1 \\Rightarrow \\frac{\\mathrm{d}g(z)}{dz} \\rightarrow 0 $ \n",
    "    - When $z$ is very **small** (say $z = -10$): $g(z)\\rightarrow -1 \\Rightarrow \\frac{\\mathrm{d}g(z)}{dz} \\rightarrow 0 $ \n",
    "    - When $z$ is **close to 0** (say $z = 0$): $g(z) = 0 \\Rightarrow \\frac{\\mathrm{d}g(z)}{dz} = 1$ \n",
    "\n",
    "- For *ReLU* and *Leaky ReLU* function:\n",
    "    - For *ReLU*:\n",
    "        $$\n",
    "        \\boxed{\n",
    "            \\!\\begin{aligned}\n",
    "            &\\quad g(z) = \\max(0,z) \\\\\n",
    "            &\\frac{\\mathrm{d}g(z)}{dz} = \\begin{cases}\n",
    "            0 (z \\le 0)\\\\\n",
    "            1 (z > 0)\n",
    "            \\end{cases}\n",
    "            \\end{aligned}\n",
    "        }\n",
    "        $$\n",
    "    - For *leaky ReLU*:\n",
    "        $$\n",
    "        \\boxed{\n",
    "            \\!\\begin{aligned}\n",
    "            &\\quad g(z) = \\max(0.01z,z) \\\\\n",
    "            &\\frac{\\mathrm{d}g(z)}{dz} = \\begin{cases}\n",
    "            0.01 (z \\le 0)\\\\\n",
    "            1 (z > 0)\n",
    "            \\end{cases}\n",
    "            \\end{aligned}\n",
    "        }\n",
    "        $$\n",
    "\n",
    "        - **Note**: taking $\\frac{\\mathrm{d}g(z)}{dz} = 0$ or $0.01$ at $z = 0$ is a technical application. \n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Gradient descent for neural networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Parameters**\n",
    "    - $w^{[i]}$, shape = $(n^{[i]}, n^{[i-1]})$ (when $i = 1$: $n^{[i-1]}$ becomes $n_x$, i.e., number of *features*)\n",
    "    - $b^{[i]}$, shape = $(n^{[i]}, 1)$\n",
    "- **Cost function**:\n",
    "    - $J(w^{[1]}, b^{[1]}, ..., w^{[l]}, b^{[l]}) = \\frac{1}{m}\\sum\\limits_{i=1}^{m} L(\\hat{y},y)$ (totally *l* layers)\n",
    "- **Gradient descent**:\n",
    "    - Repeat {\n",
    "    - compute predictions $\\hat{y}^{(i)}, i = 1-m$ (*m* samples)\n",
    "    - $\\mathrm{d}w^{[i]} = \\frac{\\partial J}{\\partial w^{[i]}}, \\mathrm{d}b^{[i]} = \\frac{\\partial J}{\\partial b^{[i]}}$\n",
    "    - $w^{[i]} := w^{[i]} - \\mathrm{d}w^{[i]}, b^{[i]} := b^{[i]} - \\mathrm{d}b^{[i]}$\n",
    "    - until parameters converge\n",
    "    - }\n",
    "- **for binary classification with one hidden layer on $n_x$ features and *m* samples**\n",
    "    - **Number of layers**: \n",
    "        - $l = 2$\n",
    "    - **Parameters**:\n",
    "        - $w^{[1]}$, shape =$(n^{[1]}, n_x)$\n",
    "        - $b^{[1]}$, shape = $(n^{[1]}, 1)$\n",
    "        - $w^{[2]}$, shape =$(1, n^{[1]})$\n",
    "        - $b^{[2]}$, shape = $(1, 1)$\n",
    "    - **Nodes of each layer**:\n",
    "        - *layer 0 (input layer)*: $X = a^{[0]}$, shape = $(n_x, m)$\n",
    "        - *layer 1 (hidden layer)*: $a^{[1]}$, shape = $(n^{[1]},m)$\n",
    "        - *layer 2 (output layer)*: $a^{[2]} = \\hat{Y}$, shape = $(1,m)$\n",
    "    - **Cost function**:\n",
    "        - $J(w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]}) = \\frac{1}{m}\\sum\\limits_{i=1}^{m} L(a^{[2]},y)$\n",
    "    - **Forward propagation**:\n",
    "        - $\\textbf{Z}^{[1]} = \\textbf{w}^{[1]T}\\textbf{X} + \\textbf{b}^{[1]}$\n",
    "        - $\\textbf{A}^{[1]} = g^{[1]}(\\textbf{Z}^{[1]})$\n",
    "        - $\\textbf{Z}^{[2]} = \\textbf{w}^{[2]T}\\textbf{A}^{[1]} + \\textbf{b}^{[2]}$\n",
    "        - $\\textbf{A}^{[2]} = \\sigma(\\textbf{Z}^{[2]})$\n",
    "\n",
    "    - **back propagation**:\n",
    "        - $\\mathrm{d}\\textbf{Z}^{[2]} = \\textbf{A}^{[2]} - \\textbf{Y}$\n",
    "        - $\\mathrm{d}\\textbf{w}^{[2]} = \\frac{1}{m}\\mathrm{d}\\textbf{Z}^{[2]}\\textbf{A}^{[1]T}$\n",
    "        - $\\mathrm{d}\\textbf{b}^{[2]} = \\frac{1}{m}\\mathrm{np.sum}(\\mathrm{d}\\textbf{Z}^{[2]})$\n",
    "        - $\\mathrm{d}\\textbf{Z}^{[1]} = \\textbf{w}^{[2]T}\\mathrm{d}\\textbf{Z}^{[2]}\\bigodot \\frac{\\mathrm{d}g^{[1]}(\\textbf{Z}^{[1]})}{\\mathrm{d}\\textbf{Z}^{[1]}}$\n",
    "        - $\\mathrm{d}\\textbf{w}^{[1]} = \\frac{1}{m}\\mathrm{d}\\textbf{Z}^{[1]}\\textbf{X}^{T}$\n",
    "        - $\\mathrm{d}\\textbf{b}^{[1]} = \\frac{1}{m}\\mathrm{np.sum}(\\mathrm{d}\\textbf{Z}^{[1]})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Random initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What happens if you initialize weights $w$ to zero?**\n",
    "    - It causes a ***symmetry breaking problem***.\n",
    "    - Say that for the *neural network* model with two *nodes* in one *hidden layer*\n",
    "        - the initial parameters are:\n",
    "            - $w^{[1]} = \\left[\\begin{array}{c}\n",
    "            0 & 0 \\\\\n",
    "            0 & 0\n",
    "            \\end{array}\\right]$\n",
    "          - $b^{[1]} = \\left[\\begin{array}{c}\n",
    "            0 \\\\\n",
    "             0\n",
    "            \\end{array}\\right]$\n",
    "    - During iteration, the hidden units $a_1^{[1]}$ and $a_1^{[2]}$ will have the same $w$ and $b$ value sets to **X**\n",
    "        - because they share the <u>same</u> *propagation process* on the <u>same</u> set of *parameters*\n",
    "\n",
    "- **Solution**:\n",
    "    - Set *parameters* to random values\n",
    "        - $w^{[1]} = 0.01* \\textrm{np.random.randn((2,2))}$ # generates a small *Gaussian* random variable in a (2,2) matrix\n",
    "        - $b^{[1]} = \\textrm{np.zero((2,1))}$ # doesn't have the *symmetry breaking problem*\n",
    "        - $w^{[2]} = 0.01* \\textrm{np.random.randn((1,2))}$\n",
    "        - $b^{[2]} = \\textrm{np.zero((1,1))}$\n",
    "    - As long as $w$ is started randomly, the *hidden units* are set differently\n",
    "\n",
    "- **What happens if $w$ are initiated with very large values?**\n",
    "    - the activation value $z$ will be very large and are highly likely to be in the large-value zone\n",
    "    - for *sigmoid* and *tanh* functions, the *slope* in this zone is very low\n",
    "    - resulting in a slow *learning process*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
