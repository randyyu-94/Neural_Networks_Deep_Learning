{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module 2: Basics of Neural Networks**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section I Logistic Regression as a Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Binary Classification\n",
    "\n",
    "- **Concept**: A classifier that can produces a label *y* with 0 (absent) or 1 (present) from a list of feature set *x* with *n* features\n",
    "- ***Notation***: \n",
    "    - (x,y), $x \\in \\textbf{R}^{n_{x}}$, $y \\in \\{0,1\\}$\n",
    "        - **for *m* training example**: ($x^{(1)}$,$y^{(1)}$), ($x^{(2)}$,$y^{(2)}$),... ($x^{(m)}$,$y^{(m)}$)\n",
    "    - $X = [x^{(1)},x^{(2)},...x^{(m)}]$, where $x^{(i)} = \\left[\\begin{array}{c}\n",
    "x^{(i)}_1\\\\\n",
    "x^{(i)}_2\\\\\n",
    "\\vdots\\\\\n",
    "x^{(i)}_{n_x}\n",
    "\\end{array}\\right]; X \\in \\textbf{R}^{{n_{x}} \\times m}, X.shape = (n_{x},m)$\n",
    "    - $y = [y^{(1)},y^{(2)},...y^{(m)}], y \\in \\{0,1\\}^{1 \\times m}, y.shape = (1,m)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Logistic Regression\n",
    "\n",
    "- **Concept**: Given *x* ($x \\in \\textbf{R}^{n_{x}}$), produce $\\hat{y}$, where $\\hat{y} = P(y=1|x)$\n",
    "- **Parameter**: $w \\in \\textbf{R}^{n_{x}}, b \\in \\textbf{R}$\n",
    "- **Output**: $\\hat{y} = \\sigma (w^{T}x+b) = \\sigma (z)$\n",
    "    - $z = w^{T}x + b$\n",
    "    - $\\sigma (z) = \\frac{1}{1+e^{-z}}$\n",
    "        - If $z \\rightarrow +\\infty \\Rightarrow e^{-z} \\rightarrow 0 \\Rightarrow \\sigma (z) \\rightarrow 1$\n",
    "        - If $z \\rightarrow -\\infty \\Rightarrow e^{-z} \\rightarrow +\\infty \\Rightarrow \\sigma (z) \\rightarrow 0$\n",
    "    \n",
    "![Sigmoid](resource%20database%20for%20MD%20notes/Week2/1280px-Logistic-curve.svg.png )  \n",
    "$\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad$*Sigmoid Function*  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Logistic Regression Cost Function\n",
    "\n",
    "- **Loss(error) function**: $L(\\hat{y},y) = -[y\\log\\hat{y}+(1-y)\\log(1-\\hat{y})]$\n",
    "    - ***Meaning***: a function to measure how good our output $\\hat{y}$ is when the true label is $y$.\n",
    "        - If $y = 1$: $L(\\hat{y},y) = -\\log\\hat{y} \\Rightarrow \\hat{y}_+\\rightarrow 1 \\leftrightarrow L(\\hat{y},y) \\rightarrow 0$\n",
    "        - If $y = 0$: $L(\\hat{y},y) = -\\log(1-\\hat{y}) \\Rightarrow \\hat{y}_-\\rightarrow 0 \\leftrightarrow L(\\hat{y},y) \\rightarrow 0$\n",
    "    - ***Application***: on a single training sample of the whole set\n",
    "- **Cost function**: $J(w,b) = \\frac{1}{m}\\sum\\limits_{i=1}^{m} L(\\hat{y}^{(i)},y^{(i)}) = -\\frac{1}{m} \\sum\\limits_{i=1}^{m} [y^{(i)}\\log\\hat{y}^{(i)}+(1-y^{(i)})\\log(1-\\hat{y}^{(i)})]$\n",
    "    - ***Application***: on the whole sample set\n",
    "    - ***Characteristic***: $\\underline{convex}$, has a minimal value with a set of *w* and *b* (named **global optimum**)\n",
    "- **Task**: Given $\\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}),... (x^{(m)},y^{(m)})\\}$, want $\\hat{y}^{(i)} \\approx y^{(i)}$\n",
    "    - ***Specifically***: Find minimized **cost function** *J* for *w* and *b* factors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Gradient Descent\n",
    "\n",
    "- **Use**: a method to calculate **global optimum** of a model\n",
    "- **Procedures** (only considering *w*):\n",
    "    - 1-1. $w_{new} = w - \\alpha\\frac{\\partial J(w,b)}{\\partial w}$\n",
    "    - 1-2. $b_{new} = b - \\alpha\\frac{\\partial J(w,b)}{\\partial b}$\n",
    "    - 2. *If not* converged [$J(w_{new})< \\mathrm{ all\\ other\\ }J(w)$]:\n",
    "        - 3. *Return to* Step 1\n",
    "    - 4. *Else* converged:\n",
    "        - 5-1. $w_{final} = w_{new}$\n",
    "        - 5-2. $b_{final} = b_{new}$\n",
    "- **Learning rate $\\alpha$**: controls how big a step is taken on each iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Derivatives (Basic)\n",
    "\n",
    "- **Meaning**: the derivative of a function is the slope of the function at a certain point and can vary at different points on the function\n",
    "- **General functions to calculate derivatives**:\n",
    "    - $f(x) = ax^b \\Rightarrow f'(x) = abx^{b-1}$\n",
    "    - $f(x) = \\log_{a}x \\Rightarrow f'(x) = \\frac{1}{x\\ln a} $\n",
    "        - *Special case*: $f(x) = \\ln x \\Rightarrow f'(x) = \\frac{1}{x} $\n",
    "    - $f(x) = a^x \\Rightarrow f'(x) = a^x \\ln a$\n",
    "        - *Special case*: $f(x) = e^x \\Rightarrow f'(x) = e^x$\n",
    "    - $f(x) = \\sin x \\Rightarrow f'(x) = \\cos x$\n",
    "    - $f(x) = \\cos x \\Rightarrow f'(x) = -\\sin x$\n",
    "    - $f(x) = \\tan x \\Rightarrow f'(x) = \\frac{1}{\\cos^2 x}$\n",
    "    - $f(x) = \\cot x \\Rightarrow f'(x) = -\\frac{1}{\\sin^2 x}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Computational Graph\n",
    "\n",
    "- **Use**: Calculate a function (e.g.,  **cost function** *J*) step-by-step from left to right in a graph\n",
    "- **Example**: Given $J = 3(a+bc)=3(a+u)=3v$\n",
    "![Computational graph](resource%20database%20for%20MD%20notes/Week2/Computational_graph.png)\n",
    "    - ***Derivative***: \n",
    "        - $\\frac{\\mathrm{d}J}{\\mathrm{d}v} = 3$ - one-step backward propagation\n",
    "        - $\\frac{\\partial J}{\\partial a} = \\frac{\\mathrm{d}J}{\\mathrm{d}v}\\frac{\\partial v}{\\partial a} = 3\\times 1 = 3$\n",
    "        - $\\frac{\\partial J}{\\partial b} = \\frac{\\mathrm{d}J}{\\mathrm{d}v}\\frac{\\partial v}{\\partial u}\\frac{\\partial u}{\\partial b} =3\\times 1\\times c = 3c$\n",
    "- **Notation in code**: $\\frac{\\mathrm{d}J}{\\mathrm{d}x}$ in code is directly denoted as $\\mathrm{d}x$ to reduce complexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Logistic Regression Gradient Descent\n",
    "\n",
    "- **For two-feature set $x_1$ and $x_2$**:\n",
    "    - **On a single training sample**:  \n",
    "$L(a,y) = -[y\\log a+(1-y)\\log(1-a)]$\n",
    "![Alt text](resource%20database%20for%20MD%20notes/Week2/LogisticRegression_derivatives.png) \n",
    "        - *Stepwise propagation*:\n",
    "            - $\\mathrm{d}a\\cancel{\\frac{\\mathrm{d}L(a,y)}{\\mathrm{d}a}} = -\\frac{y}{a}+\\frac{1-y}{1-a}$\n",
    "            - $\\mathrm{d}z\\cancel{\\frac{\\mathrm{d}L}{\\mathrm{d}z}} = \\frac{\\mathrm{d}L}{\\mathrm{d}a}\\frac{\\mathrm{d}a}{\\mathrm{d}z} = a - y$\n",
    "\n",
    "            - $\\mathrm{d}w_1\\cancel{\\frac{\\mathrm{d}L}{\\mathrm{d}w_1}} = \\frac{\\mathrm{d}L}{\\mathrm{d}a}\\frac{\\mathrm{d}a}{\\mathrm{d}z}\\frac{\\mathrm{d}z}{\\mathrm{d}w_1} = x_1 (a - y)$\n",
    "            - $\\mathrm{d}w_2\\cancel{\\frac{\\mathrm{d}L}{\\mathrm{d}w_2}} = \\frac{\\mathrm{d}L}{\\mathrm{d}a}\\frac{\\mathrm{d}a}{\\mathrm{d}z}\\frac{\\mathrm{d}z}{\\mathrm{d}w_2} = x_2 (a - y)$\n",
    "            - $\\mathrm{d}b\\cancel{\\frac{\\mathrm{d}L}{\\mathrm{d}b}} = \\frac{\\mathrm{d}L}{\\mathrm{d}a}\\frac{\\mathrm{d}a}{\\mathrm{d}z}\\frac{\\mathrm{d}z}{\\mathrm{d}b} = a - y$\n",
    "\n",
    "        - *Computation in gradient descent*:\n",
    "            - $w_1 := w_1 - \\alpha \\mathrm{d}w_1$\n",
    "            - $w_2 := w_2 - \\alpha \\mathrm{d}w_2$\n",
    "            - $b := b - \\alpha \\mathrm{d}b$\n",
    "    - **On *m* examples**:  \n",
    "$J(w,b) = \\frac{1}{m}\\sum\\limits_{i=1}^{m} L(a^{(i)},y^{(i)}) = -\\frac{1}{m} \\sum\\limits_{i=1}^{m} [y^{(i)}\\log a^{(i)}+(1-y^{(i)})\\log(1- a^{(i)})]$  \n",
    "$a^{(i)} = \\hat{y}^{(i)} = \\sigma(w^Tx^{(i)}+b)$\n",
    "        - *Stepwise propagation*:\n",
    "            - $\\mathrm{d}w_1\\cancel{\\frac{\\partial J(w,b)}{\\partial w_1}} = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\cancel{\\frac{\\partial L(a^{(i)},y^{(i)})}{\\partial w_i}}\\mathrm{d}w_1^{(i)}$\n",
    "        - *Computation in gradient descent*:\n",
    "            - $w_1 := w_1 - \\alpha \\mathrm{d}w_1$\n",
    "            - $w_2 := w_2 - \\alpha \\mathrm{d}w_2$\n",
    "            - $b := b - \\alpha \\mathrm{d}b$\n",
    "- **Overall working flow**:\n",
    "    - 1. Set: $J = 0, \\mathrm{d}w_1 = 0, \\mathrm{d}w_2 = 0, \\mathrm{d}b = 0$\n",
    "    - 2. *for* $i = 1\\ to\\ m$:\n",
    "        - 3-1. $z^{(i)} = w^Tx^{(i)}+b$\n",
    "        - 3-2. $a^{(i)} = \\sigma (z^{(i)})$\n",
    "        - 3-3. $J\\ +=\\ y^{(i)}\\log a^{(i)} + (1-y^{(i)})\\log(1-a^{(i)}) $\n",
    "        - 3-4. $\\mathrm{d}z^{(i)} = a^{(i)} - y^{(i)}$\n",
    "        - 3-5. $\\mathrm{d}w_1\\ +=\\ x_1^{(i)}\\mathrm{d}z^{(i)}$\n",
    "        - 3-6. $\\mathrm{d}w_2\\ +=\\ x_2^{(i)}\\mathrm{d}z^{(i)}$ - If more than 2 features: another *for* loop\n",
    "        - 3-7. $\\mathrm{d}b\\ +=\\ \\mathrm{d}z^{(i)}$\n",
    "    - 4-1. $J := J/m$ - Key step: get *J* value to determine the performance\n",
    "    - 4-2. $\\mathrm{d}w_1 := \\mathrm{d}w_1/m$\n",
    "    - 4-3. $\\mathrm{d}w_2 := \\mathrm{d}w_2/m$\n",
    "    - 4-4. $\\mathrm{d}b := \\mathrm{d}b/m$\n",
    "    - 5-1. $w_1 := w_1 - \\alpha\\mathrm{d}w_1$\n",
    "    - 5-2. $w_2 := w_2 - \\alpha\\mathrm{d}w_2$\n",
    "    - 5-3. $b := b - \\alpha\\mathrm{d}b$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
