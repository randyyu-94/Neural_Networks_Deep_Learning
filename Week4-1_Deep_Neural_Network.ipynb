{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module 4: Deep Neural Network**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Deep neural network representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Deep *L*-layer neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![layers](resource%20database%20for%20MD%20notes/Week4/comparing_DL_layers.png)\n",
    "*<p style=\"color:grey\" align=\"center\">comparing neural networks with different numbers of layers</p>*\n",
    "- **shallow vs. deep**: determined by the degree of *layer number*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Matrix dimensions of deep neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![4layer NN](resource%20database%20for%20MD%20notes/Week4/4layer_NN.png)\n",
    "*<p style=\"color:grey\" align=\"center\">a 4-layer neural network</p>*\n",
    "- **Number of layers**: $L = 4$\n",
    "- **Number of nodes**\n",
    "    - The input ($0^{th}$) layer (input *features*): $n^{[0]} = n_x = 3$\n",
    "    - Hidden layers:\n",
    "        - The $1^{st}$ layer: $n^{[1]} = 5$\n",
    "        - The $2^{nd}$ layer: $n^{[2]} = 5$\n",
    "        - The $3^{rd}$ layer: $n^{[3]} = 3$\n",
    "    - The output ($4^{th}$, i.e., $L$) layer: $n^{[4]} = n^{[L]} = 1$\n",
    "- **Activation function**\n",
    "    - $\\textbf{a}^{[l]} = g^{[l]}(\\textbf{z}^{[l]}), \\textbf{z}^{[l]} = \\textbf{w}^{[l]}\\textbf{a}^{[l-1]}+\\textbf{b}^{[l]}$\n",
    "    - **Note**: $\\textbf{a}^{[0]} = x, \\textbf{a}^{[L]} = \\hat{y}$\n",
    "- **Parameters of activation function**\n",
    "    - For a *single* sample:\n",
    "        - $\\textbf{z}^{[l]}$.shape =$\\textbf{a}^{[l]}$.shape = $(n^{[l]},1)$\n",
    "        - $\\mathrm{d}\\textbf{z}^{[l]}$.shape =$\\mathrm{d}\\textbf{a}^{[l]}$.shape = $(n^{[l]},1)$\n",
    "        - $\\textbf{w}^{[l]}$.shape = $(n^{[l]},n^{[l-1]})$\n",
    "        - $\\textbf{b}^{[l]}$.shape = $(n^{[l]},1)$\n",
    "    - For *m* samples (vectorized):\n",
    "        - $\\textbf{Z}^{[l]}$.shape =$\\textbf{A}^{[l]}$.shape = $(n^{[l]},m)$\n",
    "        - $\\mathrm{d}\\textbf{Z}^{[l]}$.shape =$\\mathrm{d}\\textbf{A}^{[l]}$.shape = $(n^{[l]},m)$\n",
    "        - $\\textbf{w}^{[l]}$.shape = $(n^{[l]},n^{[l-1]})$\n",
    "        - $\\textbf{b}^{[l]}$.shape = $(n^{[l]},m) (\\mathrm{broadcasted\\ from}\\ (n^{[l]},1))$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Rationale of using deep neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Intuition about deep representation**\n",
    "![Intuition of DL](resource%20database%20for%20MD%20notes/Week4/intuition_DL.png)\n",
    "*<p style=\"color:grey\" align=\"center\">Consider a neural network for idenfitying figures</p>*\n",
    "    - Early *hidden layers*: identify *low-level* features (e.g., direction of the edges)\n",
    "    - Middle *hidden layers*: compose *low-level* features and identify *mid-level* features (e.g., basic units of the face like eyes, chins, etc.)\n",
    "    - Late *hidden layers*: compose *mid-level* features and identify *high-level features (e.g., different types of faces)\n",
    "- **Circuit theory**\n",
    "    - *Informal definition*: there are functions you can compute with a small but deep *l*-layer (<u>fewer hidden units, more layers</u>) neural network that shallower networks (<u>limited layers</u>) require exponentially more hidden units to compute\n",
    "    - *Elaboration*:\n",
    "        - For a computation of *n* numbers of x (i.e., $x_1, x_2, ..., x_n$) using *XOR* computation:\n",
    "            - Using multiple *hidden layers*: the number of layers for computation is $o(\\log{(n)})$ with the maximum number of hidden unit (*layer* 1) of each layer to be $n/2$\n",
    "            - Using one *single hidden layer*: the number of hidden units for computation is $2^{(n-1)}$\n",
    "        - The complexity using *more layers* is much simpler than *more hidden units* when $n$ is very large"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Forward and backward propagation of deep neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Forward and backward functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each layer *l* for a *single* sample:\n",
    "    - **Given parameters**: $\\textbf{w}^{[l]}, \\textbf{b}^{[l]}$\n",
    "    - **Forward propagation**:\n",
    "        - **Input**: $\\textbf{a}^{[l-1]}$\n",
    "        - **Output**:$\\textbf{a}^{[l]}$\n",
    "        - **Computation**: $\\textbf{a}^{[l]} = g^{[l]}(\\textbf{z}^{[l]}), \\textbf{z}^{[l]} = \\textbf{w}^{[l]}\\textbf{a}^{[l-1]}+\\textbf{b}^{[l]}$\n",
    "    - **Backward propagation**:\n",
    "        - **Input**: $\\mathrm{d}\\textbf{a}^{[l]}$\n",
    "        - **Output**:$\\mathrm{d}\\textbf{a}^{[l-1]},\\mathrm{d}\\textbf{w}^{[l]},\\mathrm{d}\\textbf{b}^{[l]}$\n",
    "        - **Computation**: $\\mathrm{d}\\textbf{z}^{[l]}=\\mathrm{d}\\textbf{a}^{[l]}\\bigodot g^{[l]'}(\\textbf{z}^{[l]}),\\mathrm{d}\\textbf{w}^{[l]} =\\mathrm{d}\\textbf{z}^{[l]}\\textbf{a}^{[l-1]T},\\mathrm{d}\\textbf{b}^{[l]}=\\mathrm{d}\\textbf{z}^{[l]},\\mathrm{d}\\textbf{a}^{[l-1]}=\\textbf{w}^{[l]T}\\mathrm{d}\\textbf{z}^{[l]}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4layer NN](resource%20database%20for%20MD%20notes/Week4/4layer_NN.png)\n",
    "*<p style=\"color:grey\" align=\"center\">using this 4-layer neural network as an example</p>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Forward propagation: Computing a deep neural network's output on a *single* sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For *layer 1*:\n",
    "\n",
    "$$\\boxed{\\left\\{\\begin{array}{c}\n",
    "z_1^{[1]} = \\textbf{w}_1^{[1]}\\textbf{x} + b_1^{[1]}\\\\\n",
    "z_2^{[1]} = \\textbf{w}_2^{[1]}\\textbf{x} + b_2^{[1]}\\\\\n",
    "z_3^{[1]} = \\textbf{w}_3^{[1]}\\textbf{x} + b_3^{[1]}\\\\\n",
    "z_4^{[1]} = \\textbf{w}_4^{[1]}\\textbf{x} + b_4^{[1]}\\\\\n",
    "z_5^{[1]} = \\textbf{w}_5^{[1]}\\textbf{x} + b_5^{[1]}\n",
    "\\end{array}\\right\\}\\Rightarrow \\left\\{\\begin{array}{c}\n",
    "a_1^{[1]} = \\sigma{z_1^{[1]}}\\\\\n",
    "a_2^{[1]} = \\sigma{z_2^{[1]}}\\\\\n",
    "a_3^{[1]} = \\sigma{z_3^{[1]}}\\\\\n",
    "a_4^{[1]} = \\sigma{z_4^{[1]}}\\\\\n",
    "a_5^{[1]} = \\sigma{z_5^{[1]}}\n",
    "\\end{array}\\right\\}}$$\n",
    "\n",
    "$$\\boxed{\\textbf{z}^{[1]} = \\textbf{w}^{[1]}\\textbf{x} + \\textbf{b}^{[1]},\\textbf{a}^{[1]} = g^{[1]}({\\textbf{z}^{[1]}})}$$\n",
    "where: $\\textbf{z}^{[1]}$.shape = (5,1),$\\textbf{a}^{[1]}$.shape = (5,1),$\\textbf{w}^{[1]}$.shape = (5,3),$\\textbf{x}$.shape = (3,1),$\\textbf{b}^{[1]}$.shape = (5,1)\n",
    "\n",
    "- For *layer 2*:\n",
    "$$\\boxed{\\textbf{z}^{[2]} = \\textbf{w}^{[2]}\\textbf{a}^{[1]} + \\textbf{b}^{[2]},\\textbf{a}^{[2]} = g^{[2]}({\\textbf{z}^{[2]}})}$$\n",
    "where: $\\textbf{z}^{[2]}$.shape = (5,1),$\\textbf{a}^{[2]}$.shape = (5,1),$\\textbf{w}^{[2]}$.shape = (5,5),$\\textbf{b}^{[2]}$.shape = (5,1)\n",
    "\n",
    "- For *layer 3*:\n",
    "$$\\boxed{\\textbf{z}^{[3]} = \\textbf{w}^{[3]}\\textbf{a}^{[2]} + \\textbf{b}^{[3]},\\textbf{a}^{[3]} = g^{[3]}({\\textbf{z}^{[3]}})}$$\n",
    "where: $\\textbf{z}^{[3]}$.shape = (3,1),$\\textbf{a}^{[3]}$.shape = (3,1),$\\textbf{w}^{[3]}$.shape = (3,5),$\\textbf{b}^{[3]}$.shape = (3,1)\n",
    "\n",
    "\n",
    "- For *layer 4* (*output layer*):\n",
    "$$\\boxed{z^{[L]} = \\textbf{w}^{[4]}\\textbf{a}^{[3]} + \\textbf{b}^{[4]},\\hat{y}=a^{[L]} = g^{[4]}({\\textbf{z}^{[L]}})}$$\n",
    "where: $z^{[L]}$.shape = (1,1),$a^{[L]}$.shape = (1,1),$\\textbf{w}^{[4]}$.shape = (1,3),$\\textbf{b}^{[4]}$.shape = (1,1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Forward propagation: Vectorizing across *m* examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boxed{\n",
    "    \\!\\begin{aligned}\n",
    "    \\textbf{Z}^{[1]} = \\textbf{w}^{[1]}\\textbf{X} + \\textbf{b}^{[1]},\\textbf{A}^{[1]} = g^{[1]}({\\textbf{Z}^{[1]}})\\\\\n",
    "    \\textbf{Z}^{[2]} = \\textbf{w}^{[2]}\\textbf{A}^{[1]} + \\textbf{b}^{[2]},\\textbf{A}^{[2]} = g^{[2]}({\\textbf{Z}^{[2]}})\\\\\n",
    "    \\textbf{Z}^{[3]} = \\textbf{w}^{[3]}\\textbf{A}^{[2]} + \\textbf{b}^{[3]},\\textbf{A}^{[3]} = g^{[3]}({\\textbf{Z}^{[3]}})\\\\\n",
    "    \\textbf{Z}^{[L]} = \\textbf{w}^{[4]}\\textbf{A}^{[3]} + \\textbf{b}^{[4]},\\hat{\\textbf{Y}}=\\textbf{A}^{[L]} = g^{[4]}({\\textbf{Z}^{[L]}})\n",
    "    \\end{aligned}\n",
    "    }\n",
    "$$\n",
    "where: $\\textbf{Z}^{[l]}$.shape = $(n^{[l]},m)$,$\\textbf{A}^{[l]}$.shape = $(n^{[l]},m)$,$\\textbf{w}^{[l]}$.shape = $(n^{[l]},n^{[l-1]})$,$\\textbf{b}^{[l]}$.shape = $(n^{[l]},m) (\\mathrm{broadcasted\\ from}\\ (n^{[l]},1))$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (4) Forward propagation: for *layer l*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**: $\\textbf{A}^{[l-1]}$\n",
    "- **Output**: $\\textbf{A}^{[l]}$ (cache: $\\textbf{Z}^{[l]}$)\n",
    "- **Computation**: $\\textbf{A}^{[l]} = g^{[l]}(\\textbf{Z}^{[l]}), \\textbf{Z}^{[l]} = \\textbf{w}^{[l]}\\textbf{A}^{[l-1]}+\\textbf{b}^{[l]}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (5) Backward propagation: for layer *l*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**: $\\mathrm{d}\\textbf{A}^{[l]}$\n",
    "- **Output**: $\\mathrm{d}\\textbf{A}^{[l-1]},\\mathrm{d}\\textbf{w}^{[l]},\\mathrm{d}\\textbf{b}^{[l]}$\n",
    "- **Computation**: $\\mathrm{d}\\textbf{Z}^{[l]} = \\mathrm{d}\\textbf{A}^{[l]}\\bigodot g^{[l]'}(\\textbf{Z}^{[l]}), \\mathrm{d}\\textbf{w}^{[l]} = \\frac{1}{m}\\mathrm{d}\\textbf{Z}^{[l]}\\mathrm{d}\\textbf{A}^{[l-1]T},\\mathrm{d}\\textbf{w}^{[l]} = \\frac{1}{m}\\sum\\mathrm{d}\\textbf{Z}^{[l]},\\mathrm{d}\\textbf{A}^{[l-1]}=\\textbf{w}^{[l]T}\\mathrm{d}\\textbf{Z}^{[l]}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (6) General work flow for a neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![workflow](resource%20database%20for%20MD%20notes/Week4/FP_BP.JPG)\n",
    "*<p style=\"color:grey\" align=\"center\">work flow for neural network</p>*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Parameters and hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Parameters of neural network**\n",
    "    - $\\textbf{w}$ and $\\textbf{b}$\n",
    "- **Hyperparameters of neural network**\n",
    "    - *Learning rate*: $\\alpha$\n",
    "        - Determines how the parameters evolve (fast or slow)\n",
    "    - *Number of iterations* for *gradient descent*\n",
    "    - *Number of hidden layers*: *L*\n",
    "    - *Number of nodes of each layer*: $n^{[l]}$\n",
    "    - *Choice of activation function*: $g^{[l]}$ (e.g., tanh, sigmoid, ReLU)\n",
    "- **Function of hyperparameters**\n",
    "    - Control the *ultimate* and *evolution process* of *parameters*\n",
    "- **Other hyperparameters (*future course*)**\n",
    "    - Momentum term\n",
    "    - Mini batch size\n",
    "    - Regularization parameters\n",
    "- **How to tune hyperparameters**\n",
    "    - Applying DL is highly empirical and relies on \"idea-code-experiment\" cycle\n",
    "    - The final solution is based on the performance of the model\n",
    "        - Rate of converging and performance in avoiding being diverged\n",
    "        - Lowest cost function *J* with reasonable learning speed\n",
    "    - The hyperparameters may vary with *tasks* and even change with *progress of a single task*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
